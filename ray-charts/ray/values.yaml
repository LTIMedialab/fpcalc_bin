# Default values for Ray.

# RayCluster settings:

# image is Ray image to use for the head and workers of this Ray cluster.
# It's recommended to build custom dependencies for your workload into this image,
# taking one of the offical `rayproject/ray` images as base.
#image: rayproject/ray:1.11.0-e2e-py39
image: 864365278736.dkr.ecr.us-east-2.amazonaws.com/ray_deployment:0.2 # custom ray image
# headPodType is the podType used for the Ray head node (as configured below).
headPodType: rayHeadType
# podTypes is the list of pod configurations available for use as Ray nodes.
podTypes:
    # The key for each podType is a user-defined string.
    # Since we set headPodType: rayHeadType, the Ray head pod will use the configuration
    # defined in this entry of podTypes:
    rayHeadType:
        # CPU is the number of CPUs used by this pod type.
        # (Used for both requests and limits. Must be an integer, as Ray does not support fractional CPUs.)
        CPU: 40
        # memory is the memory used by this Pod type.
        # (Used for both requests and limits.)
        memory: 80Gi
        # GPU is the number of NVIDIA GPUs used by this pod type.
        # (Optional, requires GPU nodes with appropriate setup. See https://docs.ray.io/en/master/cluster/kubernetes-gpu.html)
        GPU: 0
        # rayResources is an optional string-int mapping signalling additional resources to Ray.
        # "CPU", "GPU", and "memory" are filled automatically based on the above settings, but can be overriden;
        # For example, rayResources: {"CPU": 0} can be used in the head podType to prevent Ray from scheduling tasks on the head.
        # See https://docs.ray.io/en/master/advanced.html#dynamic-remote-parameters for an example of usage of custom resources in a Ray task.
        rayResources: {}
        # Optionally, set a node selector for this podType: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
        nodeSelector: {eks.amazonaws.com/nodegroup: eks-nodes-group-ray}

        # tolerations for Ray pods of this podType (the head's podType in this case)
        #   ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
        #   Note that it is often not necessary to manually specify tolerations for GPU
        #   usage on managed platforms such as AKS, EKS, and GKE.
        #   ref: https://docs.ray.io/en/master/cluster/kubernetes-gpu.html
        tolerations: []
        # - key: "nvidia.com/gpu"
        #   operator: Exists
        #   effect: NoSchedule

    # The key for each podType is a user-defined string.
    rayWorkerType:
        # minWorkers is the minimum number of Ray workers of this pod type to keep running.
        minWorkers: 1
        # maxWorkers is the maximum number of Ray workers of this pod type to which Ray will scale.
        maxWorkers: 2
        # memory is the memory used by this Pod type.
        # (Used for both requests and limits.)
        memory: 80Gi
        # CPU is the number of CPUs used by this pod type.
        # (Used for both requests and limits. Must be an integer, as Ray does not support fractional CPUs.)
        CPU: 40
        # GPU is the number of NVIDIA GPUs used by this pod type.
        # (Optional, requires GPU nodes with appropriate setup. See https://docs.ray.io/en/master/cluster/kubernetes-gpu.html)
        GPU: 0
        # rayResources is an optional string-int mapping signalling additional resources to Ray.
        # "CPU", "GPU", and "memory" are filled automatically based on the above settings, but can be overriden;
        # For example, rayResources: {"CPU": 0} can be used in the head podType to prevent Ray from scheduling tasks on the head.
        # See https://docs.ray.io/en/master/advanced.html#dynamic-remote-parameters for an example of usage of custom resources in a Ray task.
        rayResources: {}
        # Optionally, set a node selector for this Pod type. See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
        nodeSelector: {eks.amazonaws.com/nodegroup: eks-nodes-group-ray}

        # tolerations for Ray pods of this podType
        #   ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
        #   Note that it is often not necessary to manually specify tolerations for GPU
        #   usage on managed platforms such as AKS, EKS, and GKE.
        #   ref: https://docs.ray.io/en/master/cluster/kubernetes-gpu.html
        tolerations: []
        # - key: nvidia.com/gpu
        #   operator: Exists
        #   effect: NoSchedule

    # Optionally, define more worker podTypes
    # rayWorkerType2:
    #   minWorkers: 0
    #   maxWorkers: 10
    #   memory: ...







# List if shell commands to run to set up nodes.
#initialization_commands: ["sudo apt install ffmpeg", "sudo apt install libmediainfo-dev", "sudo apt install libchromaprint-tools --assume-yes"]






# Operator settings:

# operatorOnly - If true, will only set up the Operator with this release,
# without launching a Ray cluster.
operatorOnly: false
# clusterOnly - If true, will only create a RayCluster resource with this release,
# without setting up the Operator.
# (Useful when launching multiple Ray clusters.)
clusterOnly: false
# namespacedOperator - If true, the operator is scoped to the Release namespace
# and only manages RayClusters in that namespace.
# By default, the operator is cluster-scoped and runs in the default namespace.
namespacedOperator: false
# operatorNamepsace - If using a cluster-scoped operator (namespacedOperator: false), set the namespace
# in which to launch the operator.
operatorNamespace: default
# operatorImage - The image used in the operator deployment.
# It is recommended to use one of the official `rayproject/ray` images for the operator.
# It is recommended to use the same Ray version in the operator as in the Ray clusters managed
# by the operator. In other words, the images specified under the fields `operatorImage` and `image`
# should carry matching Ray versions.
operatorImage: 864365278736.dkr.ecr.us-east-2.amazonaws.com/ray_deployment:build-8ba98df5-ab7f-42e2-9fd2-bb7765763902
